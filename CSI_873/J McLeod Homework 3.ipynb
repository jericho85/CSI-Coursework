{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jericho McLeod\n",
    "### Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms:<br>\n",
    "$\\langle \\overrightarrow{x},\\overrightarrow{t} \\rangle =$ training example, where $\\overrightarrow{x}$ is the vector of network input and $\\overrightarrow{t}$ is the vector of target network output values.<br>\n",
    "$\\eta =$ learning rate (e.g. 0.5, or some other small value from 0 to 1) <br>\n",
    "$x_{ji} =$ the input from $unit_i$ to $unit_j$ (For $unit_j$, the input $x$ comes from $unit_i$, hence $x_{ji}$)<br>\n",
    "$w_{ji} =$ the weight from $unit_i$ to $unit_j$<br>\n",
    "$n_{in} =$ the number of network inputs<br>\n",
    "$n_{out} =$ the number of network outputs<br>\n",
    "$n_{hidden} =$ the number of units in the hidden layer<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:<br>\n",
    "Create a feed-forward network with $n_{in}$ inputs, $n_{hidden}$ hidden units, and $n_{out}$ output units<br>\n",
    "Initialize all network weights to small random numbers (e.g. between -0.5 and 0.5)<br>\n",
    "Until the termination condition is met:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For each $\\langle \\overrightarrow{x},\\overrightarrow{t} \\rangle$  in $training\\_examples$, do:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the input forward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Input instance $\\overrightarrow{x}$ to the network and compute the output of $o_u$ of every unit $u$ in the network<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the errors backward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) For each network output unit $k$, calculate its error term $\\delta_k$:<br>\n",
    "$$\\delta_k \\leftarrow o_k(1-o_k)(t_k-o_k)$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) For each hidden unit $h$, calculate its error terms $\\delta_h$:\n",
    "$$\\delta_h \\leftarrow o_h(1-o_h) \\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4a) Update each network weight $w_{ji}$\n",
    "$$w_{ji} \\leftarrow w_{ji}+\\bigtriangleup w_{ji}$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where\n",
    "$$\\bigtriangleup w_{ji} = \\eta \\delta_j x_{ji}$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4b) Adding momentum changes the delta equation such that:\n",
    "$$\\bigtriangleup w_{ji} = \\eta \\delta_j x_{ji} + \\alpha \\bigtriangleup w_{ji} (n-1)$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This serves to make future updates depend partially on the prior updates, i.e., adds momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Citation: Thomas M. Mitchell. 1997. Machine Learning (1 ed.), Page 98, McGraw-Hill, Inc., New York, NY, USA._\n",
    "<br><br>\n",
    "Assigning Variables:<br>\n",
    "Weights reference: <br>\n",
    "0: $w_{ca}$<br>\n",
    "1: $w_{cb}$<br>\n",
    "2: $w_{c0}$<br>\n",
    "3: $w_{dc}$<br>\n",
    "4: $w_{d0}$<br>\n",
    "\n",
    "\n",
    "To solve this, we need to load data into a data structure and import any necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation 1\n",
      "a 1\n",
      "b 0\n",
      "d 1\n",
      "Observation 2\n",
      "a 0\n",
      "b 1\n",
      "d 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "training_examples = {'a':[1,0],'b':[0,1],'d':[1,0]}\n",
    "for i in range(2):\n",
    "    print('Observation',i+1)\n",
    "    for k,v in training_examples.items():\n",
    "        print(k,v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to creat a Neural Network class that contains the functions we will need to solve this problem. \n",
    "\n",
    "Note that this implementation is a first attempt, and is not able to create a neural network to any specific conditions, but only a solution to the specific problem at hand. \n",
    "\n",
    "The init function instatiates the class with several variables, including the input arrays, the weights and deltas of the weights in the network, the output and hidden layer results, and the learning and momentum rates. \n",
    "\n",
    "The print weights function is merely an output function for ease of showing the results required; the weights after the first two iterations of training.\n",
    "\n",
    "The feed-forward function passes inputs through the network using a sigmoid function for the results. Note that it uses the 'itera' variable: if this function were to be trained by looping through the dataset multiple times, this would need to be altered so that itera%length(dataset) = value to be used from the dataset, and the hidden layer and output values would need to be a single value rather than an array, as the size of the array could be massive otherwise.\n",
    "\n",
    "The back-propogation formula calculates the delta_k and delta_h values, then uses these to update the weights, along with momentum that is 0.9 * the prior iteration's delta weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, a, b, d):\n",
    "        self.input_a    = a\n",
    "        self.input_b    = b\n",
    "        self.weights_ca = 0.1\n",
    "        self.weights_cb = 0.1\n",
    "        self.weights_c0 = 0.1\n",
    "        self.weights_dc = 0.1\n",
    "        self.weights_d0 = 0.1\n",
    "        self.weights_ca_delta = 0\n",
    "        self.weights_cb_delta = 0\n",
    "        self.weights_c0_delta = 0\n",
    "        self.weights_dc_delta = 0\n",
    "        self.weights_d0_delta = 0\n",
    "        self.d          = d\n",
    "        self.output     = []\n",
    "        self.hidden_layer = []\n",
    "        self.learning_rate = 0.3 # eta in the formula\n",
    "        self.alpha = 0.9\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print('Weight CA',self.weights_ca)\n",
    "        print('Weight CB',self.weights_cb)\n",
    "        print('Weight C0',self.weights_c0)\n",
    "        print('Weight DC',self.weights_dc)\n",
    "        print('Weight D0',self.weights_d0)\n",
    "\n",
    "\n",
    "    def feed_forward(self,itera):\n",
    "        output_calc = lambda x: 1/(1+math.e**-x) #sigmoid\n",
    "        temp_hidden_val = self.weights_ca * self.input_a[itera]\\\n",
    "                           + self.weights_cb * self.input_b[itera]\\\n",
    "                           + self.weights_c0\n",
    "        hidden_val = output_calc(temp_hidden_val)\n",
    "        self.hidden_layer.append(hidden_val)\n",
    "        temp_out_val = self.weights_dc * self.hidden_layer[itera] + self.weights_d0\n",
    "        out_val = output_calc(temp_out_val)\n",
    "        self.output.append(out_val)\n",
    "\n",
    "    def backprop(self,itera):\n",
    "        ca_2 = 0\n",
    "        cb_2 = 0\n",
    "        c0_2 = 0\n",
    "        dc_2 = 0\n",
    "        d0_2 = 0\n",
    "        error = []\n",
    "        hidden_error = []\n",
    "        delta_k = self.output[-1]*(1-self.output[-1])*(self.d[itera]-self.output[-1])\n",
    "        delta_h = self.hidden_layer[-1]*(1-self.hidden_layer[-1]) * (self.weights_dc * delta_k)\n",
    "        error.append(delta_k)\n",
    "        hidden_error.append(delta_h)\n",
    "        delta_ca_2 = self.learning_rate * delta_h * self.input_a[itera] + self.alpha * self.weights_ca_delta\n",
    "        delta_cb_2 = self.learning_rate * delta_h * self.input_b[itera] + self.alpha * self.weights_cb_delta\n",
    "        delta_c0_2 = self.learning_rate * delta_h * 1 + self.alpha * self.weights_c0_delta\n",
    "        delta_dc_2 = self.learning_rate * delta_k * self.hidden_layer[itera] + self.alpha * self.weights_dc_delta\n",
    "        delta_d0_2 = self.learning_rate * delta_k * 1 + self.alpha * self.weights_d0_delta\n",
    "        self.weights_ca_delta = delta_ca_2\n",
    "        self.weights_cb_delta = delta_cb_2\n",
    "        self.weights_c0_delta = delta_c0_2\n",
    "        self.weights_dc_delta = delta_dc_2\n",
    "        self.weights_d0_delta = delta_d0_2\n",
    "        self.weights_ca = self.weights_ca + self.weights_ca_delta\n",
    "        self.weights_cb = self.weights_cb + self.weights_cb_delta\n",
    "        self.weights_c0 = self.weights_c0 + self.weights_c0_delta\n",
    "        self.weights_dc = self.weights_dc + self.weights_dc_delta\n",
    "        self.weights_d0 = self.weights_d0 + self.weights_d0_delta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the neural network with the input data, run through two iterations of training, and print the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Weights\n",
      "Weight CA 0.1\n",
      "Weight CB 0.1\n",
      "Weight C0 0.1\n",
      "Weight DC 0.1\n",
      "Weight D0 0.1\n",
      "\n",
      "Weights after 1 iteration\n",
      "Weight CA 0.10085128181864877\n",
      "Weight CB 0.1\n",
      "Weight C0 0.10085128181864877\n",
      "Weight DC 0.1189103977991797\n",
      "Weight D0 0.1343929220303063\n",
      "\n",
      "Weights after 2 iterations\n",
      "Weight CA 0.10161743545543266\n",
      "Weight CB 0.09879852785432702\n",
      "Weight C0 0.10041596330975969\n",
      "Weight DC 0.11347416438338892\n",
      "Weight D0 0.12452152136346561\n"
     ]
    }
   ],
   "source": [
    "nn = ANN(training_examples['a'],training_examples['b'],training_examples['d'])\n",
    "print(\"Starting Weights\")\n",
    "nn.print_weights()\n",
    "\n",
    "print(\"\\nWeights after 1 iteration\")\n",
    "nn.feed_forward(0)\n",
    "nn.backprop(0)\n",
    "nn.print_weights()\n",
    "\n",
    "print(\"\\nWeights after 2 iterations\")\n",
    "nn.feed_forward(1)\n",
    "nn.backprop(1)\n",
    "\n",
    "nn.print_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revise the backpropagation algorithm in Table 4.2 so that it operates on units using the squashing function $tanh$  in place of the sigmoid function. That is, assume the output of a single unit is \n",
    "$o = tanh(\\overrightarrow{w} \\times \\overrightarrow{x})$. Give the weight update rule for output layer weights and hidden layer weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid output error function is $$\\delta_k \\leftarrow o_k(1-o_k)(t_k-o_k)$$<br>\n",
    "and the hidden layer error function is $$\\delta_h \\leftarrow o_h(1-o_h) \\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$\n",
    "because the sigmoid function is $$\\sigma (y) = \\frac{1}{1+e^-y}$$ and its derivative is $$\\frac{d\\sigma(y)}{dy}=\\sigma(y)\\times(1-\\sigma(y))$$\n",
    "<br><br>\n",
    "Extrapolating that the error terms are then $$\\delta_k \\leftarrow G'(k) \\times (t_k-o_k)$$ and $$\\delta_h \\leftarrow G'(k)\\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for a $tanh$ function $$\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)}$$\n",
    "for which the derivative is $$\\tanh'(x) = 1-\\tanh^2 (x)$$\n",
    "the output error function is $$\\delta_k \\leftarrow 1-\\tanh^2 (o_k)\\times(t_k-o_k)$$\n",
    "and the hidden layer output error function is $$\\delta_h \\leftarrow 1-\\tanh^2 (o_k)\\times \\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the complete algorithm for backpropagation using the $tanh$ squashing function is: <br><br>\n",
    "Algorithm:<br>\n",
    "Create a feed-forward network with $n_{in}$ inputs, $n_{hidden}$ hidden units, and $n_{out}$ output units<br>\n",
    "Initialize all network weights to small random numbers (e.g. between -0.5 and 0.5)<br>\n",
    "Until the termination condition is met:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For each $\\langle \\overrightarrow{x},\\overrightarrow{t} \\rangle$  in $training\\_examples$, do:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the input forward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Input instance $\\overrightarrow{x}$ to the network and compute the output of $o_u$ of every unit $u$ in the network<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the errors backward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) For each network output unit $k$, calculate its error term $\\delta_k$:<br>\n",
    "$$\\delta_k \\leftarrow 1-\\tanh^2 (o_k)\\times(t_k-o_k)$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) For each hidden unit $h$, calculate its error terms $\\delta_h$:\n",
    "$$\\delta_h \\leftarrow 1-\\tanh^2 (o_k)\\times \\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4a) Update each network weight $w_{ji}$\n",
    "$$w_{ji} \\leftarrow w_{ji}+\\bigtriangleup w_{ji}$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where\n",
    "$$\\bigtriangleup w_{ji} = \\eta \\delta_j x_{ji}$$<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
