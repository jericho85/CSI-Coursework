{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms:<br>\n",
    "$\\langle \\overrightarrow{x},\\overrightarrow{t} \\rangle =$ training example, where $\\overrightarrow{x}$ is the vector of network input and $\\overrightarrow{t}$ is the vector of target network output values.<br>\n",
    "$\\eta =$ learning rate (e.g. 0.5, or some other small value from 0 to 1) <br>\n",
    "$x_{ji} =$ the input from $unit_i$ to $unit_j$ (For $unit_j$, the input $x$ comes from $unit_i$, hence $x_{ji}$)<br>\n",
    "$w_{ji} =$ the weight from $unit_i$ to $unit_j$<br>\n",
    "$n_{in} =$ the number of network inputs<br>\n",
    "$n_{out} =$ the number of network outputs<br>\n",
    "$n_{hidden} =$ the number of units in the hidden layer<br>\n",
    "\n",
    "Algorithm:<br>\n",
    "Create a feed-forward network with $n_{in}$ inputs, $n_{hidden}$ hidden units, and $n_{out}$ output units<br>\n",
    "Initialize all network weights to small random numbers (e.g. between -0.5 and 0.5)<br>\n",
    "Until the termination condition is met:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For each $\\langle \\overrightarrow{x},\\overrightarrow{t} \\rangle$  in $training\\_examples$, do:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the input forward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) Input instance $\\overrightarrow{x}$ to the network and compute the output of $o_u$ of every unit $u$ in the network<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Propogate the errors backward through the network:<br><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2) For each network output unit $k$, calculate its error term $\\delta_k$:<br>\n",
    "$$\\delta_k \\leftarrow o_k(1-o_k)(t_k-o_k)$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3) For each hidden unit $h$, calculate its error terms $\\delta_h$:\n",
    "$$\\delta_h \\leftarrow o_h(1-o_h) \\sum\\limits_{k\\in outputs} w_{kh}\\delta_k$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4a) Update each network weight $w_{ji}$\n",
    "$$w_{ji} \\leftarrow w_{ji}+\\bigtriangleup w_{ji}$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;where\n",
    "$$\\bigtriangleup w_{ji} = \\eta \\delta_j x_{ji}$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4b) Adding momentum changes the delta equation such that:\n",
    "$$\\bigtriangleup w_{ji} = \\eta \\delta_j x_{ji} + \\alpha \\bigtriangleup w_{ji} (n-1)$$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This serves to make future updates depend partially on the prior updates, i.e., adds momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Citation: Thomas M. Mitchell. 1997. Machine Learning (1 ed.), Page 98, McGraw-Hill, Inc., New York, NY, USA._\n",
    "<br><br>\n",
    "Assigning Variables:<br>\n",
    "Weights reference: <br>\n",
    "0: $w_{ca}$<br>\n",
    "1: $w_{cb}$<br>\n",
    "2: $w_{c0}$<br>\n",
    "3: $w_{dc}$<br>\n",
    "4: $w_{d0}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation 1\n",
      "a 1\n",
      "b 0\n",
      "d 1\n",
      "Observation 2\n",
      "a 0\n",
      "b 1\n",
      "d 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "weights = [.1,.1,.1,.1,.1]\n",
    "learning_rate = 0.3\n",
    "alpha = 0.9\n",
    "training_examples = {'a':[1,0],'b':[0,1],'d':[1,0]}\n",
    "for i in range(2):\n",
    "    print('Observation',i+1)\n",
    "    for k,v in training_examples.items():\n",
    "        print(k,v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, a, b, d):\n",
    "        self.input_a    = a\n",
    "        self.input_b    = b\n",
    "        self.weights_ca = 0.1\n",
    "        self.weights_cb = 0.1\n",
    "        self.weights_c0 = 0.1\n",
    "        self.weights_dc = 0.1\n",
    "        self.weights_d0 = 0.1\n",
    "        self.weights_ca_delta = 0\n",
    "        self.weights_cb_delta = 0\n",
    "        self.weights_c0_delta = 0\n",
    "        self.weights_dc_delta = 0\n",
    "        self.weights_d0_delta = 0\n",
    "        self.d          = d\n",
    "        self.output     = []\n",
    "        self.hidden_layer = []\n",
    "\n",
    "\n",
    "    def feed_forward(self,itera):\n",
    "        output_calc = lambda x: 1/(1+math.e**-x)\n",
    "        #for i in range(len(self.input_a)):\n",
    "        temp_hidden_val = self.weights_ca * self.input_a[itera]\\\n",
    "                           + self.weights_cb * self.input_b[itera]\\\n",
    "                           + self.weights_c0\n",
    "        hidden_val = output_calc(temp_hidden_val)\n",
    "        self.hidden_layer.append(hidden_val)\n",
    "        temp_out_val = self.weights_dc * self.hidden_layer[itera] + self.weights_d0\n",
    "        out_val = output_calc(temp_out_val)\n",
    "        self.output.append(out_val)\n",
    "        print('output',self.output)\n",
    "        print('hidden',self.hidden_layer)\n",
    "        print('a',self.input_a)\n",
    "        print('b',self.input_b)\n",
    "        print('d',self.d)\n",
    "\n",
    "    def backprop(self):\n",
    "        ca_2 = 0\n",
    "        cb_2 = 0\n",
    "        c0_2 = 0\n",
    "        dc_2 = 0\n",
    "        d0_2 = 0\n",
    "        error = []\n",
    "        # 𝛿𝑘←𝑜𝑘(1−𝑜𝑘)(𝑡𝑘−𝑜𝑘)\n",
    "        # ℎ←𝑜ℎ(1−𝑜ℎ)∑𝑘∈𝑜𝑢𝑡𝑝𝑢𝑡𝑠 𝑤𝑘ℎ 𝛿𝑘\n",
    "        hidden_error = []\n",
    "        for i in range(len(self.output)):\n",
    "            delta_k = self.output[i]*(1-self.output[i])*(self.d[i]-self.output[i])\n",
    "            delta_h = self.hidden_layer[i]*(1-self.hidden_layer[i]) * (self.weights_dc * delta_k)\n",
    "            print('delta h',delta_h)\n",
    "            print('delta k',delta_k)\n",
    "            error.append(delta_k)\n",
    "            hidden_error.append(delta_h)\n",
    "            #print(delta_k,delta_h)\n",
    "        \n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        #d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        #d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        #self.weights1 += d_weights1\n",
    "        #self.weights2 += d_weights2\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output [0.5386684799635422]\n",
      "hidden [0.549833997312478]\n",
      "a [1, 0]\n",
      "b [0, 1]\n",
      "d [1, 0]\n",
      "delta h 0.0028376060621625463\n",
      "delta k 0.11464307343435433\n"
     ]
    }
   ],
   "source": [
    "nn = ANN(training_examples['a'],training_examples['b'],training_examples['d'])\n",
    "nn.feed_forward(0)\n",
    "nn.backprop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
